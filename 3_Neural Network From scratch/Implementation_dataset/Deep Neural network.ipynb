{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_df= pd.read_csv('titanic/train.csv')\n",
    "test_df= pd.read_csv('titanic/test.csv')\n",
    "\n",
    "def clean(df):\n",
    "    df['Age'].fillna(train_df[\"Age\"].median(skipna=True), inplace= True)\n",
    "    df[\"Embarked\"].fillna(train_df[\"Embarked\"].value_counts().idxmax(), inplace=True)\n",
    "    df.drop(\"Cabin\", axis=1, inplace=True)\n",
    "    df.drop(\"Name\", axis=1, inplace=True)\n",
    "    df.drop(\"Ticket\", axis=1, inplace=True)\n",
    "    df.drop(\"PassengerId\",axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "a= train_df.copy()\n",
    "b= test_df.copy()\n",
    "\n",
    "train= clean(a)\n",
    "test= clean(b)\n",
    "\n",
    "li= [round(num) for num in train['Fare']]\n",
    "train['Fare']=li\n",
    "\n",
    "li= [round(num) for num in train['Age']]\n",
    "train['Age']=li\n",
    "\n",
    "train['Sex'] =train['Sex'].map({'male':1,'female':0})\n",
    "train['Embarked'] =train['Embarked'].map({'S':0,'C':1,'Q':2})\n",
    "\n",
    "#-------------------------\n",
    "test= test.fillna(train['Fare'].median())\n",
    "\n",
    "lu= [round(num) for num in test['Fare']]\n",
    "test['Fare']=lu\n",
    "\n",
    "lu= [round(num) for num in test['Age']]\n",
    "test['Age']=lu\n",
    "\n",
    "test['Sex'] =test['Sex'].map({'male':1,'female':0})\n",
    "test['Embarked'] =test['Embarked'].map({'S':0,'C':1,'Q':2})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X= train.drop('Survived',axis=1).to_numpy().T # 7,891 -- (input,n_examples)\n",
    "Y= pd.DataFrame(train['Survived']).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Structure\n",
    "    def initialize_parameters_deep(layers_dims):\n",
    "        ...\n",
    "        return parameters \n",
    "    def L_model_forward(X, parameters):\n",
    "        ...\n",
    "        return AL, caches\n",
    "    def compute_cost(AL, Y):\n",
    "        ...\n",
    "        return cost\n",
    "    def L_model_backward(AL, Y, caches):\n",
    "        ...\n",
    "        return grads\n",
    "    def update_parameters(parameters, grads, learning_rate):\n",
    "        ...\n",
    "        return parameters\n",
    "    def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "        for i in range(0, num_iterations):\n",
    "\n",
    "            AL, caches = L_model_forward(X,parameters)\n",
    "            cost = compute_cost(AL,Y)\n",
    "\n",
    "            grads = L_model_backward(AL,Y,caches)\n",
    "\n",
    "            parameters = update_parameters(parameters,grads,learning_rate)\n",
    "\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  1,  3, ...,  3,  1,  3],\n",
       "       [ 1,  0,  0, ...,  0,  1,  1],\n",
       "       [22, 38, 26, ..., 28, 26, 32],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  2,  0,  0],\n",
       "       [ 7, 71,  8, ..., 23, 30,  8],\n",
       "       [ 0,  1,  0, ...,  0,  1,  2]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layers_dims):\n",
    "    parameters={}\n",
    "    L= len(layers_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)]= np.random.randn(layers_dims[l],layers_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)]= np.zeros((layers_dims[l],1))        \n",
    "        \n",
    "    return parameters \n",
    "\n",
    "# layers_dims=[7,12,6,4,1]\n",
    "# parameters=initialize_parameters_deep(layers_dims)\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    Z= np.dot(W,A) +b\n",
    "    cache={\n",
    "        'A':A,\n",
    "        'W':W,\n",
    "        'b':b\n",
    "    }\n",
    "    return Z,cache\n",
    "\n",
    "def relu(Z):\n",
    "    A= np.maximum(0.1,Z)\n",
    "    cache= Z\n",
    "    return A,cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A= 1 / (1 + np.exp(-Z))\n",
    "    return A,Z\n",
    "\n",
    "\n",
    "def linear_activation_forward(A,W,b,activation):\n",
    "    if activation == 'relu':\n",
    "        Z,linear_cache = linear_forward(A,W,b)\n",
    "        A,activation_cache= relu(Z)    \n",
    "    elif activation =='sigmoid':\n",
    "        Z,linear_cache= linear_forward(A,W,b)\n",
    "        A,activation_cache= sigmoid(Z)\n",
    "        \n",
    "    cache= (linear_cache,activation_cache)\n",
    "    return A,cache # cache contain A,W,b,Z\n",
    "\n",
    "\n",
    "def L_model_forward(A, parameters):\n",
    "    L= len(parameters)//2\n",
    "    caches=[]\n",
    "    for l in range(1,L):\n",
    "        A_prev= A\n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "\n",
    "    AL,cache= linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "# AL,caches = L_model_forward(X.T, parameters)\n",
    "# print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m= Y.shape[0]\n",
    "    cost= -(1/m) * ( np.dot(np.log(AL),Y) + np.dot(np.log(1-AL), 1-Y)  )\n",
    "    cost= float(np.squeeze(cost))\n",
    "    return cost               \n",
    "\n",
    "# cost= compute_cost(AL,Y)\n",
    "# cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    A_prev= cache['A']\n",
    "    W = cache['W']\n",
    "    b = cache['b']\n",
    "    \n",
    "    dW= np.dot(dZ,A_prev.T)\n",
    "    dB= np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev= np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev,dW,dB\n",
    "\n",
    "def sigmoid_backward(dAL,Z):\n",
    "    A = 1/ (1+ np.exp(-Z))\n",
    "    g_prime= A * (1-A)\n",
    "    dZ= dAL * g_prime\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dAL,Z):\n",
    "    A= np.maximum(0,Z)\n",
    "    g_prime= np.where(A <= 0, 0, 1)\n",
    "    dZ= A * g_prime\n",
    "\n",
    "    return dZ\n",
    "    \n",
    "def linear_activation_backward(dAL, caches, activation):\n",
    "    linear_cache,activation_cache = caches\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ= relu_backward(dAL,activation_cache)\n",
    "        dA_prev,dW,dB= linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ= sigmoid_backward(dAL,activation_cache)\n",
    "        dA_prev,dW,dB= linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    return dA_prev,dW,dB\n",
    "        \n",
    "def L_model_backward(AL,Y,caches):\n",
    "    L= len(caches)\n",
    "    grads={}\n",
    "    dAL= - np.divide(Y.T,AL)- np.divide(1-Y.T,1-AL)\n",
    "    grads['dA'+str(L)],grads['dW'+str(L)],grads['dB'+str(L)] =linear_activation_backward(dAL,caches[L-1],'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(1,L)):\n",
    "        dA_prev,dW,dB= linear_activation_backward(grads['dA'+str(l+1)],caches[l-1],'relu')\n",
    "        grads['dA'+str(l)]= dA_prev\n",
    "        grads['dW'+str(l)]= dW\n",
    "        grads['dB'+str(l)]= dB\n",
    "    return grads\n",
    "\n",
    "    \n",
    "    \n",
    "# grads= L_model_backward(AL,Y,caches)\n",
    "# grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    L= len(parameters)//2\n",
    "    for l in range(L-1):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate * grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate * grads['dB'+str(l+1)]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# update_parameters(parameters,grads,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    }
   ],
   "source": [
    "%xmode Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.692686444239014\n",
      "1 0.692686444239014\n",
      "2 0.692686444239014\n",
      "3 0.692686444239014\n",
      "4 0.692686444239014\n",
      "5 0.692686444239014\n",
      "6 0.692686444239014\n",
      "7 0.692686444239014\n",
      "8 0.692686444239014\n",
      "9 0.692686444239014\n"
     ]
    }
   ],
   "source": [
    "def L_layer_model(X,Y,layer_dims,learning_rate,num_iteration):\n",
    "    parameters= initialize_parameters_deep(layer_dims)\n",
    "    \n",
    "    for i in range(num_iteration):\n",
    "        AL,caches= L_model_forward(X,parameters)\n",
    "        \n",
    "        cost= compute_cost(AL,Y)\n",
    "        \n",
    "        grads= L_model_backward(AL,Y,caches)\n",
    "        parameters= update_parameters(parameters,grads,learning_rate)\n",
    "        print(i,cost)\n",
    "#         if i ==1:\n",
    "#             print('parameters :',parameters.keys())\n",
    "#             print('grads :',grads.keys())\n",
    "#             break\n",
    "#         else:\n",
    "\n",
    "#             print('parameters :',parameters.keys())\n",
    "#             print('grads :',grads.keys())\n",
    "#             print('-'*100)\n",
    "            \n",
    "    \n",
    "\n",
    "layer_dims=[7,12,6,4,1]\n",
    "L_layer_model(X,Y,layer_dims,2.1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
